message = paste("Error accessing GEO dataset:", e$message)
))
}
})
}
#' Detect array platform from GEO metadata
#'
#' @param platforms Vector of platform IDs
#' @param title Dataset title
#' @param summary Dataset summary
#' @return Detected array type
#' @keywords internal
detect_array_platform <- function(platforms, title = "", summary = "") {
# Platform ID mappings
platform_map <- list(
"GPL13534" = "450k",    # HumanMethylation450 BeadChip
"GPL8490" = "450k",     # HumanMethylation450 BeadChip
"GPL21145" = "EPIC",    # HumanMethylationEPIC BeadChip
"GPL23976" = "EPICv2"   # HumanMethylationEPIC v2.0 BeadChip
)
# Check platform IDs first
for (platform in platforms) {
if (platform %in% names(platform_map)) {
return(platform_map[[platform]])
}
}
# Fallback to text analysis
combined_text <- paste(title, summary, collapse = " ")
if (grepl("epicv2|epic.?v2|epic.?2", combined_text, ignore.case = TRUE)) {
return("EPICv2")
} else if (grepl("epic|850k", combined_text, ignore.case = TRUE)) {
return("EPIC")
} else if (grepl("450k|450", combined_text, ignore.case = TRUE)) {
return("450k")
}
# Default to EPIC if unsure but looks like methylation
return("EPIC")
}
#' Download and process GEO methylation dataset
#'
#' @description
#' Downloads IDAT files and phenotypic data from a GEO dataset and processes
#' them for GIMP analysis.
#'
#' @param geo_id Character string of GEO accession (e.g., "GSE12345")
#' @param download_dir Directory to download files (default: temporary directory)
#' @param group_column Column name for sample groups (auto-detect if NULL)
#' @param control_terms Terms that indicate control samples
#' @param case_terms Terms that indicate case/treatment samples
#' @param max_samples Maximum number of samples to process (NULL for all)
#' @param normalize_method Normalization method for minfi
#' @param n_cores Number of CPU cores for parallel processing
#'
#' @return List containing processed beta matrix and sample information
#' @examples
#' \donttest{
#' # Process a GEO dataset
#' geo_data <- process_geo_dataset(
#'   geo_id = "GSE68777",
#'   group_column = "characteristics_ch1.1",
#'   control_terms = c("normal", "control"),
#'   case_terms = c("tumor", "cancer")
#' )
#'
#' # Use with GIMP functions
#' ICRcpg <- make_cpgs(Bmatrix = geo_data$beta_matrix, bedmeth = "v1")
#' }
#' @export
process_geo_dataset <- function(geo_id,
download_dir = NULL,
group_column = NULL,
control_terms = c("control", "normal", "healthy", "ctrl"),
case_terms = c("case", "disease", "tumor", "cancer", "patient"),
max_samples = NULL,
normalize_method = "quantile",
n_cores = NULL) {
# Validate prerequisites
if (!requireNamespace("GEOquery", quietly = TRUE)) {
stop("GEOquery package is required. Install with: BiocManager::install('GEOquery')")
}
# Validate dataset first
validation <- validate_geo_dataset(geo_id)
if (!validation$valid) {
stop("Dataset validation failed: ", validation$message)
}
if (!validation$has_idats) {
stop("Dataset does not contain IDAT files. GIMP requires raw IDAT data.")
}
message("=== PROCESSING GEO DATASET: ", geo_id, " ===")
message("Title: ", validation$title)
message("Array type: ", validation$array_type)
message("Sample count: ", validation$sample_count)
# Setup directories
if (is.null(download_dir)) {
download_dir <- file.path(tempdir(), paste0("geo_", geo_id))
}
if (!dir.exists(download_dir)) {
dir.create(download_dir, recursive = TRUE)
}
idat_dir <- file.path(download_dir, "idats")
if (!dir.exists(idat_dir)) {
dir.create(idat_dir, recursive = TRUE)
}
# Download and extract data
message("\n=== DOWNLOADING GEO DATA ===")
tryCatch({
# Get detailed GSE information
gse <- GEOquery::getGSE(geo_id, destdir = download_dir, GSEMatrix = TRUE)
# Extract phenotypic data from the first (usually only) series matrix
if (length(gse) > 1) {
message("Multiple series detected, using the first one")
}
eset <- gse[[1]]
pheno_data <- Biobase::pData(eset)
message("Phenotypic data extracted: ", nrow(pheno_data), " samples, ",
ncol(pheno_data), " variables")
# Download supplementary files (IDAT files)
message("\n=== DOWNLOADING IDAT FILES ===")
supp_files <- GEOquery::getGEOSuppFiles(geo_id, baseDir = download_dir,
fetch_files = TRUE)
# Extract any compressed files
extract_compressed_files(file.path(download_dir, geo_id))
# Find and organize IDAT files
idat_files <- find_and_organize_idats(file.path(download_dir, geo_id), idat_dir)
if (length(idat_files) == 0) {
stop("No IDAT files found in downloaded supplementary data")
}
message("Found ", length(idat_files), " IDAT files")
# Create sample sheet from phenotypic data
message("\n=== CREATING SAMPLE SHEET ===")
sample_sheet <- create_geo_sample_sheet(
pheno_data = pheno_data,
idat_files = idat_files,
group_column = group_column,
control_terms = control_terms,
case_terms = case_terms,
max_samples = max_samples
)
message("Sample sheet created with ", nrow(sample_sheet), " samples")
# Save sample sheet
sample_sheet_path <- file.path(idat_dir, "samplesheet.csv")
write.csv(sample_sheet, sample_sheet_path, row.names = FALSE)
# Create ZIP file for compatibility with existing GIMP functions
message("\n=== CREATING ZIP FOR PROCESSING ===")
zip_path <- create_geo_zip(idat_dir, download_dir, geo_id)
# Process with existing GIMP IDAT pipeline
message("\n=== PROCESSING WITH GIMP PIPELINE ===")
# Map GEO array type to GIMP bedmeth parameter
bedmeth_mapping <- list(
"450k" = "450k",
"EPIC" = "v1",
"EPICv2" = "v2"
)
idat_results <- read_idat_zip(
zip_file = zip_path,
sample_sheet_name = "samplesheet.csv",
array_type = validation$array_type,
normalize_method = normalize_method,
detection_pval = 0.01,
remove_failed_samples = TRUE,
n_cores = n_cores
)
# Add GEO-specific metadata
idat_results$geo_metadata <- list(
geo_id = geo_id,
title = validation$title,
array_type = validation$array_type,
original_sample_count = validation$sample_count,
processed_sample_count = ncol(idat_results$beta_matrix),
download_dir = download_dir,
group_detection = sample_sheet$group_detection_info[1]
)
message("\n=== GEO PROCESSING COMPLETED ===")
message("✅ Successfully processed ", ncol(idat_results$beta_matrix), " samples")
message("✅ Beta matrix dimensions: ", paste(dim(idat_results$beta_matrix), collapse = " x "))
return(idat_results)
}, error = function(e) {
# Cleanup on error
if (dir.exists(download_dir)) {
unlink(download_dir, recursive = TRUE)
}
stop("GEO processing failed: ", e$message, call. = FALSE)
})
}
#' Extract compressed files in a directory
#'
#' @param dir_path Directory to search for compressed files
#' @keywords internal
extract_compressed_files <- function(dir_path) {
# Find compressed files
tar_files <- list.files(dir_path, pattern = "\\.(tar|tar\\.gz|tgz)$",
full.names = TRUE, recursive = TRUE)
zip_files <- list.files(dir_path, pattern = "\\.zip$",
full.names = TRUE, recursive = TRUE)
# Extract tar files
for (tar_file in tar_files) {
message("Extracting: ", basename(tar_file))
utils::untar(tar_file, exdir = dirname(tar_file))
}
# Extract zip files
for (zip_file in zip_files) {
message("Extracting: ", basename(zip_file))
utils::unzip(zip_file, exdir = dirname(zip_file))
}
}
#' Find and organize IDAT files
#'
#' @param source_dir Directory to search for IDAT files
#' @param target_dir Directory to copy organized IDAT files
#' @return Vector of IDAT file paths
#' @keywords internal
find_and_organize_idats <- function(source_dir, target_dir) {
# Find all IDAT files recursively
idat_files <- list.files(source_dir, pattern = "\\.idat$",
full.names = TRUE, recursive = TRUE, ignore.case = TRUE)
if (length(idat_files) == 0) {
return(character(0))
}
# Copy IDAT files to organized directory
for (idat_file in idat_files) {
file.copy(idat_file, file.path(target_dir, basename(idat_file)), overwrite = TRUE)
}
# Return list of organized files
list.files(target_dir, pattern = "\\.idat$", full.names = TRUE)
}
#' Create sample sheet from GEO phenotypic data
#'
#' @param pheno_data Phenotypic data from GEO
#' @param idat_files Vector of IDAT file paths
#' @param group_column Column name for groups
#' @param control_terms Terms indicating controls
#' @param case_terms Terms indicating cases
#' @param max_samples Maximum samples to include
#' @return Data frame with sample sheet
#' @keywords internal
create_geo_sample_sheet <- function(pheno_data, idat_files, group_column,
control_terms, case_terms, max_samples) {
# Extract sample identifiers from IDAT files
idat_basenames <- unique(gsub("_(Red|Grn)\\.idat$", "", basename(idat_files)))
# Match samples between phenotypic data and IDAT files
# Try multiple matching strategies
sample_matches <- match_geo_samples(pheno_data, idat_basenames)
if (nrow(sample_matches) == 0) {
stop("Could not match any samples between phenotypic data and IDAT files")
}
message("Matched ", nrow(sample_matches), " samples between pheno data and IDAT files")
# Limit samples if requested
if (!is.null(max_samples) && nrow(sample_matches) > max_samples) {
sample_matches <- sample_matches[1:max_samples, ]
message("Limited to first ", max_samples, " samples")
}
# Detect or use specified group column
if (is.null(group_column)) {
group_info <- auto_detect_groups(pheno_data, sample_matches$pheno_id,
control_terms, case_terms)
group_column <- group_info$column
sample_groups <- group_info$groups
group_detection_msg <- group_info$message
} else {
if (!group_column %in% colnames(pheno_data)) {
stop("Specified group column '", group_column, "' not found in phenotypic data")
}
sample_groups <- assign_sample_groups(pheno_data[sample_matches$pheno_id, group_column],
control_terms, case_terms)
group_detection_msg <- paste("Used specified column:", group_column)
}
# Create sample sheet
sample_sheet <- data.frame(
Sample_Name = sample_matches$pheno_id,
Sentrix_ID = extract_sentrix_info(sample_matches$idat_id)$sentrix_id,
Sentrix_Position = extract_sentrix_info(sample_matches$idat_id)$sentrix_position,
Sample_Group = sample_groups,
GEO_Sample = sample_matches$pheno_id,
IDAT_Basename = sample_matches$idat_id,
group_detection_info = group_detection_msg,
stringsAsFactors = FALSE
)
# Validate sample sheet
valid_rows <- !is.na(sample_sheet$Sample_Name) &
!is.na(sample_sheet$Sentrix_ID) &
!is.na(sample_sheet$Sentrix_Position)
sample_sheet <- sample_sheet[valid_rows, ]
message("Group assignment: ", table(sample_sheet$Sample_Group))
message("Group detection: ", group_detection_msg)
return(sample_sheet)
}
#' Match samples between phenotypic data and IDAT files
#'
#' @param pheno_data Phenotypic data
#' @param idat_basenames IDAT file basenames
#' @return Data frame with matched samples
#' @keywords internal
match_geo_samples <- function(pheno_data, idat_basenames) {
matches <- data.frame(
pheno_id = character(0),
idat_id = character(0),
stringsAsFactors = FALSE
)
# Strategy 1: Direct GEO sample ID matching
geo_samples <- rownames(pheno_data)
for (geo_sample in geo_samples) {
# Try exact match
exact_match <- grep(paste0("^", geo_sample, "_"), idat_basenames, value = TRUE)
if (length(exact_match) > 0) {
matches <- rbind(matches, data.frame(
pheno_id = geo_sample,
idat_id = exact_match[1],
stringsAsFactors = FALSE
))
next
}
# Try partial match
partial_match <- grep(geo_sample, idat_basenames, value = TRUE)
if (length(partial_match) > 0) {
matches <- rbind(matches, data.frame(
pheno_id = geo_sample,
idat_id = partial_match[1],
stringsAsFactors = FALSE
))
}
}
# Strategy 2: If few matches, try reverse matching
if (nrow(matches) < length(geo_samples) * 0.5) {
for (idat_base in idat_basenames) {
# Extract potential sample ID from IDAT filename
potential_id <- extract_sample_id_from_idat(idat_base)
if (potential_id %in% geo_samples && !potential_id %in% matches$pheno_id) {
matches <- rbind(matches, data.frame(
pheno_id = potential_id,
idat_id = idat_base,
stringsAsFactors = FALSE
))
}
}
}
# Remove duplicates
matches <- matches[!duplicated(matches$pheno_id), ]
matches <- matches[!duplicated(matches$idat_id), ]
return(matches)
}
#' Auto-detect sample groups from phenotypic data
#'
#' @param pheno_data Phenotypic data
#' @param sample_ids Sample IDs to analyze
#' @param control_terms Terms indicating controls
#' @param case_terms Terms indicating cases
#' @return List with group information
#' @keywords internal
auto_detect_groups <- function(pheno_data, sample_ids, control_terms, case_terms) {
# Look for promising columns
potential_columns <- grep("group|condition|treatment|disease|status|type|class",
colnames(pheno_data), ignore.case = TRUE, value = TRUE)
if (length(potential_columns) == 0) {
# Fallback to characteristics columns
potential_columns <- grep("characteristics", colnames(pheno_data),
ignore.case = TRUE, value = TRUE)
}
if (length(potential_columns) == 0) {
# Use all character/factor columns as potential group columns
potential_columns <- colnames(pheno_data)[sapply(pheno_data, function(x) {
is.character(x) || is.factor(x)
})]
}
# Score each column for group potential
best_column <- NULL
best_score <- 0
best_groups <- NULL
for (col in potential_columns) {
if (col %in% colnames(pheno_data)) {
col_data <- pheno_data[sample_ids, col]
groups <- assign_sample_groups(col_data, control_terms, case_terms)
# Score based on balance and clarity
group_table <- table(groups)
if (length(group_table) == 2) {
balance_score <- min(group_table) / max(group_table)
clarity_score <- sum(groups != "Unknown") / length(groups)
total_score <- balance_score * clarity_score
if (total_score > best_score) {
best_score <- total_score
best_column <- col
best_groups <- groups
}
}
}
}
if (is.null(best_column)) {
# Fallback: assign all as Unknown
best_groups <- rep("Unknown", length(sample_ids))
message_text <- "Could not auto-detect sample groups. Manual assignment required."
} else {
message_text <- paste("Auto-detected groups from column:", best_column,
"(", paste(names(table(best_groups)), collapse = " vs "), ")")
}
return(list(
column = best_column,
groups = best_groups,
message = message_text
))
}
#' Assign sample groups based on terms
#'
#' @param values Vector of values to classify
#' @param control_terms Terms indicating controls
#' @param case_terms Terms indicating cases
#' @return Vector of group assignments
#' @keywords internal
assign_sample_groups <- function(values, control_terms, case_terms) {
values <- as.character(values)
groups <- rep("Unknown", length(values))
# Check for control terms
for (term in control_terms) {
control_matches <- grepl(term, values, ignore.case = TRUE)
groups[control_matches] <- "Control"
}
# Check for case terms
for (term in case_terms) {
case_matches <- grepl(term, values, ignore.case = TRUE)
groups[case_matches] <- "Case"
}
return(groups)
}
#' Extract Sentrix information from IDAT basename
#'
#' @param idat_basenames Vector of IDAT basenames
#' @return List with Sentrix ID and position
#' @keywords internal
extract_sentrix_info <- function(idat_basenames) {
# Try to extract standard Sentrix format: SentrixID_SentrixPosition
sentrix_pattern <- "^(\\d+)_([A-Z]\\d+[A-Z]\\d+)$"
sentrix_ids <- character(length(idat_basenames))
sentrix_positions <- character(length(idat_basenames))
for (i in seq_along(idat_basenames)) {
basename <- idat_basenames[i]
# Try standard pattern
if (grepl(sentrix_pattern, basename)) {
matches <- regmatches(basename, regexec(sentrix_pattern, basename))[[1]]
sentrix_ids[i] <- matches[2]
sentrix_positions[i] <- matches[3]
} else {
# Fallback: create synthetic IDs
sentrix_ids[i] <- paste0("200000000", sprintf("%02d", i))
sentrix_positions[i] <- paste0("R", sprintf("%02d", ((i-1) %% 8) + 1), "C01")
}
}
return(list(
sentrix_id = sentrix_ids,
sentrix_position = sentrix_positions
))
}
#' Extract sample ID from IDAT filename
#'
#' @param idat_basename IDAT file basename
#' @return Potential sample ID
#' @keywords internal
extract_sample_id_from_idat <- function(idat_basename) {
# Remove common prefixes and suffixes
clean_name <- gsub("^.*?(GSM\\d+).*$", "\\1", idat_basename)
if (clean_name != idat_basename) {
return(clean_name)
}
# Fallback: return first part before underscore
parts <- strsplit(idat_basename, "_")[[1]]
return(parts[1])
}
#' Create ZIP file for GIMP processing
#'
#' @param idat_dir Directory containing organized IDAT files
#' @param base_dir Base directory for ZIP creation
#' @param geo_id GEO ID for naming
#' @return Path to created ZIP file
#' @keywords internal
create_geo_zip <- function(idat_dir, base_dir, geo_id) {
zip_path <- file.path(base_dir, paste0(geo_id, "_processed.zip"))
# Get all files to include
files_to_zip <- list.files(idat_dir, full.names = TRUE)
# Create ZIP file
oldwd <- getwd()
setwd(idat_dir)
tryCatch({
utils::zip(zip_path, files = basename(files_to_zip), flags = "-r9Xq")
setwd(oldwd)
}, error = function(e) {
setwd(oldwd)
stop("Failed to create ZIP file: ", e$message)
})
if (!file.exists(zip_path)) {
stop("ZIP file creation failed")
}
message("Created ZIP file: ", zip_path, " (", round(file.size(zip_path)/1024^2, 1), " MB)")
return(zip_path)
}
# Helper operator for NULL coalescing (if not already defined)
if (!exists("%||%")) {
`%||%` <- function(x, y) if (is.null(x)) y else x
}
process_geo_dataset(geo_id = "GSE68777")
View(process_geo_dataset)
View(process_geo_dataset)
roxygenise()
roxygenise()
library(roxygen2)
roxygenise()
library(GIMP)
GIMP_app()
library(roxygen2)
roxygenise()
roxygenise()
library(devtools)
devtools::install_local()
setwd("/run/media/saadat/A/R_Packages/GIMP")
setwd("/run/media/saadat/A/R_Packages/GIMP")
devtools::install_local()
